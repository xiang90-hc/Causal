{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4704530c",
   "metadata": {},
   "source": [
    "## CausalML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d806c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from causalml.inference.meta import XGBTRegressor, MLPTRegressor\n",
    "from causalml.inference.meta import BaseXRegressor\n",
    "from causalml.inference.meta.tmle import TMLELearner\n",
    "from causalml.match import NearestNeighborMatch, MatchOptimizer, create_table_one\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "from causalml.dataset import *\n",
    "from causalml.metrics import *\n",
    "from causalml.inference.tree import CausalRandomForestRegressor\n",
    "\n",
    "\n",
    "from causallearn.search.FCMBased import lingam\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.utils.cit import chisq\n",
    "from dowhy import CausalModel\n",
    "from dowhy.utils.graph_operations import adjacency_matrix_to_graph\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d9a0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 0.121\n",
      "Help on function synthetic_data in module causalml.dataset.regression:\n",
      "\n",
      "synthetic_data(mode=1, n=1000, p=5, sigma=1.0, adj=0.0)\n",
      "    Synthetic data in Nie X. and Wager S. (2018) 'Quasi-Oracle Estimation of Heterogeneous Treatment Effects'\n",
      "    Args:\n",
      "        mode (int, optional): mode of the simulation:             1 for difficult nuisance components and an easy treatment effect.             2 for a randomized trial.             3 for an easy propensity and a difficult baseline.             4 for unrelated treatment and control groups.             5 for a hidden confounder biasing treatment.\n",
      "        n (int, optional): number of observations\n",
      "        p (int optional): number of covariates (>=5)\n",
      "        sigma (float): standard deviation of the error term\n",
      "        adj (float): adjustment term for the distribution of propensity, e. Higher values shift the distribution to 0.\n",
      "                     It does not apply to mode == 2 or 3.\n",
      "    Returns:\n",
      "        (tuple): Synthetically generated samples with the following outputs:\n",
      "            - y ((n,)-array): outcome variable.\n",
      "            - X ((n,p)-ndarray): independent variables.\n",
      "            - w ((n,)-array): treatment flag with value 0 or 1.\n",
      "            - tau ((n,)-array): individual treatment effect.\n",
      "            - b ((n,)-array): expected outcome.\n",
      "            - e ((n,)-array): propensity of receiving treatment.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "y, X, treatment, tau, b, e = synthetic_data(mode=4, n=10000, p=8, sigma=1.0)\n",
    "print(\"True ATE: {:.03f}\".format(tau.mean()))\n",
    "print(help(synthetic_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126553c",
   "metadata": {},
   "source": [
    "#### X-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a088e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using XGB:\n",
      "(array([0.14295317]), array([0.11201952]), array([0.17388681]))\n",
      "ATE estimate: 0.143\n",
      "ATE lower bound: 0.112\n",
      "ATE upper bound: 0.174\n",
      "\n",
      "Using Linear Regression:\n",
      "ATE estimate: 0.068\n",
      "ATE lower bound: 0.015\n",
      "ATE upper bound: 0.120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learner_x = BaseXRegressor(learner=XGBRegressor())\n",
    "ate_x = learner_x.estimate_ate(X=X, treatment=treatment, y=y, p=e)\n",
    "print('Using XGB:')\n",
    "print(ate_x)\n",
    "\n",
    "print('ATE estimate: {:.03f}'.format(ate_x[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_x[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_x[2][0]))\n",
    "\n",
    "print()\n",
    "\n",
    "learner_x = BaseXRegressor(learner=LinearRegression())\n",
    "ate_x = learner_x.estimate_ate(X=X, treatment=treatment, y=y, p=e)\n",
    "print('Using Linear Regression:')\n",
    "print('ATE estimate: {:.03f}'.format(ate_x[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_x[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_x[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6334b5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Random Forest:\n",
      "ATE estimate: 0.238\n",
      "ATE lower bound: 0.193\n",
      "ATE upper bound: 0.284\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400,      \n",
    "    max_depth=14,         \n",
    "    min_samples_split=80,  \n",
    "    min_samples_leaf=30,   \n",
    "    max_features=\"sqrt\", \n",
    "    bootstrap=True,\n",
    "    oob_score=True, \n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "learner_x = BaseXRegressor(learner=rf)\n",
    "ate_x = learner_x.estimate_ate(X=X, treatment=treatment, y=y, p=e)\n",
    "print('Using Random Forest:')\n",
    "print('ATE estimate: {:.03f}'.format(ate_x[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_x[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_x[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4363016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP:\n",
      "ATE estimate: 0.096\n",
      "ATE lower bound: 0.048\n",
      "ATE upper bound: 0.144\n"
     ]
    }
   ],
   "source": [
    "mlp_base = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"mlp\", MLPRegressor(\n",
    "        hidden_layer_sizes=(256, 128, 64),\n",
    "        activation=\"relu\",\n",
    "        alpha=5e-5,\n",
    "        learning_rate_init=1e-3,\n",
    "        batch_size=256,\n",
    "        max_iter=600,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15,\n",
    "        n_iter_no_change=12,\n",
    "        tol=1e-4,\n",
    "        random_state=42\n",
    "\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "learner_x = BaseXRegressor(learner=mlp_base)\n",
    "ate_x = learner_x.estimate_ate(X=X, treatment=treatment, y=y, p=e)\n",
    "print('Using MLP:')\n",
    "print('ATE estimate: {:.03f}'.format(ate_x[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_x[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_x[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599ba2a",
   "metadata": {},
   "source": [
    "#### Causal Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0040cf4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[0;32m      3\u001b[0m crf \u001b[38;5;241m=\u001b[39m CausalRandomForestRegressor(\n\u001b[0;32m      4\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m      5\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m crf\u001b[38;5;241m.\u001b[39mfit(X, treatment, y)\n\u001b[0;32m     16\u001b[0m ite \u001b[38;5;241m=\u001b[39m crf\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m     17\u001b[0m ate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(ite)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\causalml\\inference\\tree\\causal\\causalforest.py:408\u001b[0m, in \u001b[0;36mCausalRandomForestRegressor.fit\u001b[1;34m(self, X, treatment, y, sample_weight)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03mFit Causal RandomForest\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m     self\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m X, y, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimator\u001b[38;5;241m.\u001b[39m_prepare_data(X\u001b[38;5;241m=\u001b[39mX, treatment\u001b[38;5;241m=\u001b[39mtreatment, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X\u001b[38;5;241m=\u001b[39mX, treatment\u001b[38;5;241m=\u001b[39mw, y\u001b[38;5;241m=\u001b[39my, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\causalml\\inference\\tree\\causal\\causalforest.py:350\u001b[0m, in \u001b[0;36mCausalRandomForestRegressor._fit\u001b[1;34m(self, X, treatment, y, sample_weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m         random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m    346\u001b[0m     trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    349\u001b[0m     ]\n\u001b[1;32m--> 350\u001b[0m     trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    351\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    352\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_joblib_parallel_args,\n\u001b[0;32m    354\u001b[0m     )(\n\u001b[0;32m    355\u001b[0m         delayed(_parallel_build_trees)(\n\u001b[0;32m    356\u001b[0m             tree\u001b[38;5;241m=\u001b[39mt,\n\u001b[0;32m    357\u001b[0m             forest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    358\u001b[0m             X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    359\u001b[0m             treatment\u001b[38;5;241m=\u001b[39mtreatment,\n\u001b[0;32m    360\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    361\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    362\u001b[0m             tree_idx\u001b[38;5;241m=\u001b[39mi,\n\u001b[0;32m    363\u001b[0m             n_trees\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    364\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    365\u001b[0m             class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    366\u001b[0m             n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    367\u001b[0m         )\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moob_score:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n=10000\n",
    "\n",
    "crf = CausalRandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=16,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=40,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True, \n",
    "    oob_score=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "crf.fit(X, treatment, y)\n",
    "\n",
    "ite = crf.predict(X)\n",
    "ate = np.mean(ite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81671e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_boot):\n\u001b[0;32m      5\u001b[0m     idx \u001b[38;5;241m=\u001b[39m rng\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(n), size\u001b[38;5;241m=\u001b[39mn, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m     ite_b \u001b[38;5;241m=\u001b[39m crf\u001b[38;5;241m.\u001b[39mpredict(X[idx])\n\u001b[0;32m      7\u001b[0m     ates\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(ite_b))\n\u001b[0;32m      8\u001b[0m ates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ates)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\causalml\\inference\\tree\\causal\\causalforest.py:427\u001b[0m, in \u001b[0;36mCausalRandomForestRegressor.predict\u001b[1;34m(self, X, with_outcomes)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 427\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:1079\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;66;03m# Parallel loop\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m lock \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m-> 1079\u001b[0m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msharedmem\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m   1080\u001b[0m     delayed(_accumulate_prediction)(e\u001b[38;5;241m.\u001b[39mpredict, X, [y_hat], lock)\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[0;32m   1082\u001b[0m )\n\u001b[0;32m   1084\u001b[0m y_hat \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_hat\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ates = []\n",
    "n_boot = 100\n",
    "rng = np.random.RandomState(42)\n",
    "for _ in range(n_boot):\n",
    "    idx = rng.choice(np.arange(n), size=n, replace=True)\n",
    "    ite_b = crf.predict(X[idx])\n",
    "    ates.append(np.mean(ite_b))\n",
    "ates = np.array(ates)\n",
    "\n",
    "ci_lower, ci_upper = np.percentile(ates, [2.5, 97.5])\n",
    "\n",
    "print(\"ATE estimate: {:.03f}\".format(ate))\n",
    "print(\"ATE lower bound: {:.03f}\".format(ci_lower))\n",
    "print(\"ATE upper bound: {:.03f}\".format(ci_upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dff431",
   "metadata": {},
   "source": [
    "#### TMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3724ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TMLE:\n",
      "ATE estimate: 0.154\n",
      "ATE lower bound: 0.101\n",
      "ATE upper bound: 0.207\n"
     ]
    }
   ],
   "source": [
    "pm = ElasticNetPropensityModel(n_fold=5, random_state=42)\n",
    "p_hat = pm.fit_predict(X, treatment)\n",
    "\n",
    "tmle = TMLELearner(learner=GradientBoostingRegressor(random_state=42))\n",
    "ate_tmle = tmle.estimate_ate(X, treatment=treatment, y=y, p=p_hat, return_ci=True)\n",
    "\n",
    "print('Using TMLE:')\n",
    "print('ATE estimate: {:.03f}'.format(ate_tmle[0][0]))\n",
    "print('ATE lower bound: {:.03f}'.format(ate_tmle[1][0]))\n",
    "print('ATE upper bound: {:.03f}'.format(ate_tmle[2][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6ec4b",
   "metadata": {},
   "source": [
    "## DoWhy + Causal_Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c941c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [f\"X{i+1}\" for i in range(X.shape[1])] + [\"T\", \"Y\"]\n",
    "df = pd.DataFrame(np.column_stack([X, treatment, y]), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e86a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiNGAM → DoWhy ATE (IPW): 0\n"
     ]
    }
   ],
   "source": [
    "model = lingam.ICALiNGAM()\n",
    "model.fit(df.values)\n",
    "A = model.adjacency_matrix_\n",
    "\n",
    "labels = list(df.columns)\n",
    "G = nx.DiGraph()\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        if A[i, j] != 0:\n",
    "            G.add_edge(labels[i], labels[j])\n",
    "\n",
    "\n",
    "causal_model = CausalModel(data=df, treatment=\"T\", outcome=\"Y\", graph=G)\n",
    "identified = causal_model.identify_effect()\n",
    "estimate = causal_model.estimate_effect(\n",
    "    identified,\n",
    "    method_name=\"backdoor.propensity_score_weighting\"\n",
    ")\n",
    "\n",
    "print(\"LiNGAM → DoWhy ATE (IPW):\", estimate.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457786a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a60cd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
